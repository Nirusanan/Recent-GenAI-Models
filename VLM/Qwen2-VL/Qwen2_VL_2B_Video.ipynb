{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoVFLt53jLq9",
        "outputId": "eb683128-05cb-4d1e-be7a-d31be4397b96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.1/315.1 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q \"transformers==4.45.0\" \"accelerate==0.33.0\" \"bitsandbytes==0.43.1\" \"safetensors>=0.3.1\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q av"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdPdIHvztm23",
        "outputId": "a58265f5-b46a-4ff7-d778-5a8d239d5396"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.0/33.0 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import base64\n",
        "import math\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import io, transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "\n",
        "IMAGE_FACTOR = 28\n",
        "MIN_PIXELS = 4 * 28 * 28\n",
        "MAX_PIXELS = 16384 * 28 * 28\n",
        "MAX_RATIO = 200\n",
        "\n",
        "VIDEO_MIN_PIXELS = 128 * 28 * 28\n",
        "VIDEO_MAX_PIXELS = 768 * 28 * 28\n",
        "VIDEO_TOTAL_PIXELS = 24576 * 28 * 28\n",
        "FRAME_FACTOR = 2\n",
        "FPS = 2.0\n",
        "FPS_MIN_FRAMES = 4\n",
        "FPS_MAX_FRAMES = 768\n",
        "\n",
        "\n",
        "def round_by_factor(number: int, factor: int) -> int:\n",
        "    \"\"\"Returns the closest integer to 'number' that is divisible by 'factor'.\"\"\"\n",
        "    return round(number / factor) * factor\n",
        "\n",
        "\n",
        "def ceil_by_factor(number: int, factor: int) -> int:\n",
        "    \"\"\"Returns the smallest integer greater than or equal to 'number' that is divisible by 'factor'.\"\"\"\n",
        "    return math.ceil(number / factor) * factor\n",
        "\n",
        "\n",
        "def floor_by_factor(number: int, factor: int) -> int:\n",
        "    \"\"\"Returns the largest integer less than or equal to 'number' that is divisible by 'factor'.\"\"\"\n",
        "    return math.floor(number / factor) * factor\n",
        "\n",
        "\n",
        "def smart_resize(\n",
        "    height: int, width: int, factor: int = IMAGE_FACTOR, min_pixels: int = MIN_PIXELS, max_pixels: int = MAX_PIXELS\n",
        ") -> tuple[int, int]:\n",
        "    \"\"\"\n",
        "    Rescales the image so that the following conditions are met:\n",
        "\n",
        "    1. Both dimensions (height and width) are divisible by 'factor'.\n",
        "\n",
        "    2. The total number of pixels is within the range ['min_pixels', 'max_pixels'].\n",
        "\n",
        "    3. The aspect ratio of the image is maintained as closely as possible.\n",
        "    \"\"\"\n",
        "    if max(height, width) / min(height, width) > MAX_RATIO:\n",
        "        raise ValueError(\n",
        "            f\"absolute aspect ratio must be smaller than {MAX_RATIO}, got {max(height, width) / min(height, width)}\"\n",
        "        )\n",
        "    h_bar = max(factor, round_by_factor(height, factor))\n",
        "    w_bar = max(factor, round_by_factor(width, factor))\n",
        "    if h_bar * w_bar > max_pixels:\n",
        "        beta = math.sqrt((height * width) / max_pixels)\n",
        "        h_bar = floor_by_factor(height / beta, factor)\n",
        "        w_bar = floor_by_factor(width / beta, factor)\n",
        "    elif h_bar * w_bar < min_pixels:\n",
        "        beta = math.sqrt(min_pixels / (height * width))\n",
        "        h_bar = ceil_by_factor(height * beta, factor)\n",
        "        w_bar = ceil_by_factor(width * beta, factor)\n",
        "    return h_bar, w_bar\n",
        "\n",
        "\n",
        "def fetch_image(ele: dict[str, str | Image.Image], size_factor: int = IMAGE_FACTOR) -> Image.Image:\n",
        "    if \"image\" in ele:\n",
        "        image = ele[\"image\"]\n",
        "    else:\n",
        "        image = ele[\"image_url\"]\n",
        "    image_obj = None\n",
        "    if isinstance(image, Image.Image):\n",
        "        image_obj = image\n",
        "    elif image.startswith(\"http://\") or image.startswith(\"https://\"):\n",
        "        image_obj = Image.open(requests.get(image, stream=True).raw)\n",
        "    elif image.startswith(\"file://\"):\n",
        "        image_obj = Image.open(image[7:])\n",
        "    elif image.startswith(\"data:image\"):\n",
        "        if \"base64,\" in image:\n",
        "            _, base64_data = image.split(\"base64,\", 1)\n",
        "            data = base64.b64decode(base64_data)\n",
        "            image_obj = Image.open(BytesIO(data))\n",
        "    else:\n",
        "        image_obj = Image.open(image)\n",
        "    if image_obj is None:\n",
        "        raise ValueError(f\"Unrecognized image input, support local path, http url, base64 and PIL.Image, got {image}\")\n",
        "    image = image_obj.convert(\"RGB\")\n",
        "    ## resize\n",
        "    if \"resized_height\" in ele and \"resized_width\" in ele:\n",
        "        resized_height, resized_width = smart_resize(\n",
        "            ele[\"resized_height\"],\n",
        "            ele[\"resized_width\"],\n",
        "            factor=size_factor,\n",
        "        )\n",
        "    else:\n",
        "        width, height = image.size\n",
        "        min_pixels = ele.get(\"min_pixels\", MIN_PIXELS)\n",
        "        max_pixels = ele.get(\"max_pixels\", MAX_PIXELS)\n",
        "        resized_height, resized_width = smart_resize(\n",
        "            height,\n",
        "            width,\n",
        "            factor=size_factor,\n",
        "            min_pixels=min_pixels,\n",
        "            max_pixels=max_pixels,\n",
        "        )\n",
        "    image = image.resize((resized_width, resized_height))\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def fetch_video(ele: dict, image_factor: int = IMAGE_FACTOR) -> torch.Tensor | list[Image.Image]:\n",
        "    if isinstance(ele[\"video\"], str):\n",
        "        # TODO: support http url\n",
        "\n",
        "        video = ele[\"video\"]\n",
        "        if video.startswith(\"file://\"):\n",
        "            video = video[7:]\n",
        "\n",
        "        video, audio, info = io.read_video(\n",
        "            video,\n",
        "            start_pts=ele.get(\"video_start\", 0.0),\n",
        "            end_pts=ele.get(\"video_end\", None),\n",
        "            pts_unit=\"sec\",\n",
        "            output_format=\"TCHW\",\n",
        "        )\n",
        "\n",
        "        assert not (\"fps\" in ele and \"nframes\" in ele), \"Only accept either `fps` or `nframes`\"\n",
        "        if \"nframes\" in ele:\n",
        "            nframes = round_by_factor(ele[\"nframes\"], FRAME_FACTOR)\n",
        "        else:\n",
        "            fps = ele.get(\"fps\", FPS)\n",
        "            min_frames = ceil_by_factor(ele.get(\"min_frames\", FPS_MIN_FRAMES), FRAME_FACTOR)\n",
        "            max_frames = floor_by_factor(ele.get(\"max_frames\", min(FPS_MAX_FRAMES, video.size(0))), FRAME_FACTOR)\n",
        "            nframes = video.size(0) / info[\"video_fps\"] * fps\n",
        "            nframes = min(max(nframes, min_frames), max_frames)\n",
        "            nframes = round_by_factor(nframes, FRAME_FACTOR)\n",
        "        if not (FRAME_FACTOR <= nframes and nframes <= video.size(0)):\n",
        "            raise ValueError(f\"nframes should in interval [{FRAME_FACTOR}, {video.size(0)}], but got {nframes}.\")\n",
        "\n",
        "        idx = torch.linspace(0, video.size(0) - 1, nframes).round().long()\n",
        "        height, width = video.shape[2:]\n",
        "        video = video[idx]\n",
        "\n",
        "        min_pixels = ele.get(\"min_pixels\", VIDEO_MIN_PIXELS)\n",
        "        total_pixels = ele.get(\"total_pixels\", VIDEO_TOTAL_PIXELS)\n",
        "        max_pixels = max(min(VIDEO_MAX_PIXELS, total_pixels / nframes * FRAME_FACTOR), int(min_pixels * 1.05))\n",
        "        max_pixels = ele.get(\"max_pixels\", max_pixels)\n",
        "        if \"resized_height\" in ele and \"resized_width\" in ele:\n",
        "            resized_height, resized_width = smart_resize(\n",
        "                ele[\"resized_height\"],\n",
        "                ele[\"resized_width\"],\n",
        "                factor=image_factor,\n",
        "            )\n",
        "        else:\n",
        "            resized_height, resized_width = smart_resize(\n",
        "                height,\n",
        "                width,\n",
        "                factor=image_factor,\n",
        "                min_pixels=min_pixels,\n",
        "                max_pixels=max_pixels,\n",
        "            )\n",
        "        video = transforms.functional.resize(\n",
        "            video,\n",
        "            [resized_height, resized_width],\n",
        "            interpolation=InterpolationMode.BICUBIC,\n",
        "            antialias=True,\n",
        "        ).float()\n",
        "        return video\n",
        "    else:\n",
        "        assert isinstance(ele[\"video\"], (list, tuple))\n",
        "        process_info = ele.copy()\n",
        "        process_info.pop(\"type\", None)\n",
        "        process_info.pop(\"video\", None)\n",
        "        images = [\n",
        "            fetch_image({\"image\": video_element, **process_info}, size_factor=image_factor)\n",
        "            for video_element in ele[\"video\"]\n",
        "        ]\n",
        "        nframes = ceil_by_factor(len(images), FRAME_FACTOR)\n",
        "        if len(images) < nframes:\n",
        "            images.extend([images[-1]] * (nframes - len(images)))\n",
        "        return images\n",
        "\n",
        "\n",
        "def extract_vision_info(conversations: list[dict] | list[list[dict]]) -> list[dict]:\n",
        "    vision_infos = []\n",
        "    if isinstance(conversations[0], dict):\n",
        "        conversations = [conversations]\n",
        "    for conversation in conversations:\n",
        "        for message in conversation:\n",
        "            if isinstance(message[\"content\"], list):\n",
        "                for ele in message[\"content\"]:\n",
        "                    if (\n",
        "                        \"image\" in ele\n",
        "                        or \"image_url\" in ele\n",
        "                        or \"video\" in ele\n",
        "                        or ele[\"type\"] in (\"image\", \"image_url\", \"video\")\n",
        "                    ):\n",
        "                        vision_infos.append(ele)\n",
        "    return vision_infos\n",
        "\n",
        "\n",
        "def process_vision_info(\n",
        "    conversations: list[dict] | list[list[dict]],\n",
        ") -> tuple[list[Image.Image] | None, list[torch.Tensor | list[Image.Image]] | None]:\n",
        "    vision_infos = extract_vision_info(conversations)\n",
        "    ## Read images or videos\n",
        "    image_inputs = []\n",
        "    video_inputs = []\n",
        "    for vision_info in vision_infos:\n",
        "        if \"image\" in vision_info or \"image_url\" in vision_info:\n",
        "            image_inputs.append(fetch_image(vision_info))\n",
        "        elif \"video\" in vision_info:\n",
        "            video_inputs.append(fetch_video(vision_info))\n",
        "        else:\n",
        "            raise ValueError(\"image, image_url or video should in content.\")\n",
        "    if len(image_inputs) == 0:\n",
        "        image_inputs = None\n",
        "    if len(video_inputs) == 0:\n",
        "        video_inputs = None\n",
        "    return image_inputs, video_inputs\n"
      ],
      "metadata": {
        "id": "sxx5TmW7jMLb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
        "import torch\n",
        "\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592,
          "referenced_widgets": [
            "ad813596e9ba43939ed06620af5c50fe",
            "000a15b41a724da1923f23264ecbef41",
            "26c2f165275447a695fae350554d861f",
            "3b2534da3afe46209a000bcaf598df09",
            "fca2922be3b442cf9459e1801f29f3da",
            "1444f5f1819045388b31fd9ad17e4d05",
            "444f156c61fb44c4932633b8b2706a74",
            "39d0730637d74f34a51e04b938aa8cbe",
            "fc72e732e46d4c74b98aec562ccddf84",
            "da05a12a408d4fb4b2080ba3c989f8d2",
            "357c3806d0c94157adf2b8b845bca730",
            "cfbf3decac3b402a819e81f48041d4f9",
            "efa97b3a89f446afaf54fabd4b092889",
            "302f93e8f3654f2d8d595283374bdda7",
            "7a9048d94c5b4b269e838d59fdc3fbe3",
            "5250e539c6fd4bf59235d07bce69fb89",
            "a58faae080874e699b5cba76a0df0be2",
            "76e6da029d0b4530a06e39b3640866e4",
            "5c65a9ff90914e51845a71e2c773bfca",
            "40778c06e79a45cc803f680e4e45a9ad",
            "96832f9a72e946d08cfe66b83a309229",
            "5472a534a8d84cb88e7a57f297234778",
            "9c629eef4f1e4cbe84920b6f2fe14202",
            "6f0ab3f2368a488a8badd23f888bcc2b",
            "14457c80c46d4e4aa78a836f77953529",
            "36a594a5f2754a78a948c274e68e1ee7",
            "44017b51b30a47bf90e346bf5c8a04f6",
            "9937adf1a7cf42c0af02bf26448e6e12",
            "f6dc219df55d4b3ea119bf7ef7ffa9ec",
            "48f62067940d4f1b8e2f5bae34a2a429",
            "226ad4b6c4dc49a5a5cd402c15a468f5",
            "fb49fa156547482992ab56e119165b54",
            "71e779e3e2804949ac68241e801288d0",
            "b003d6be599449c5b9ed4f1a8e2d4f99",
            "7b53abaeb45047cda4daa3860a599508",
            "be34822a701348ce8235379bbe770eb1",
            "28586d26970744c09633f78422bba8df",
            "5e499e5e74dd44099c6e54ed088f416e",
            "0f8bc5aac6174831ae1f5387abec6bbf",
            "737027ee1b204af39d12881ddac3a8e0",
            "0e9e8efe9d594a209f7279e755df3ce9",
            "2d8137bdfffe4ee2adc96aa1da0bab31",
            "e5ddcdf386ca4bc78cfe0773aaf1c7f0",
            "10a1a3cd2ff7481f905575e166196434",
            "5cfa91a75d8b43a183268d703c076f31",
            "335ed55dcad04f9eafb5fc8a6437f728",
            "c9ac74a070e6405e8f0d88064643121d",
            "aafdcc0f7d674391bc9f029f7af1de7d",
            "aee52ad6471a4ab28cdb2cae3a540ca8",
            "80ab37e160274a509b63d4d140257407",
            "49a1acb424b14ec79bb7464db40c51aa",
            "86d7d752746847d7a8d2cf395b86f6b7",
            "7266fe9f433c4696947fadf04bf941b6",
            "bbaab64cd92b4c6e8b256d589b603b86",
            "11868bbd553148ce9b979cef3186e999",
            "a05bb468631a4e38ace9fc28077972dc",
            "6f3facd939594e20beee7b98ccc8a20b",
            "ab2b2ebd95d64977b25c0ae68c3fd1c2",
            "12532a6e47e645b58193f5180329e4ec",
            "431b07685003476bb60a0707f96ac966",
            "c3ea7b838b5b473db60e23be3df5651b",
            "6971cd91b9274ad58b908fd56b919036",
            "e2fd556c09ce4a9a91afd8dca929b631",
            "18ecd7b515c84eddb8a67dcc916f4c2b",
            "60a1e6cf0a8b46be92fe275caafbd4ad",
            "95ea9f1d24324f66aac9681fc5db6c58",
            "06764f72b3d34cddb55c8fd742afaf88",
            "6568b74f99e84baabde4e4a7d60754c2",
            "dbe5a4a2b41d42f1b61573810ec8c279",
            "79af2d6dbae94f4093160bb3deada798",
            "7703a1b70231428780d2259d836adcd4",
            "39146c20021540878e00c6eafaabb220",
            "4628194d059d485eb9c83ed672be8d68",
            "5c0e52c70bda488c84d91aca62065eb3",
            "84b537136a3843b1aa3748aaaf3f38c7",
            "0390d17f861b46e597df0acf9809309c",
            "9a8399a39a16430f90bc1c02ee09168a",
            "9a36c6df9bbc49aebe4ed916c9d3489b",
            "d91f24f78cf541ca8d8322124e71b03e",
            "e57e83a352fb4d89850aee5fabab0de6",
            "502d67b438dc436bb0abe272d5811b35",
            "66b46a46e681471cb6ea71f98ee81aa2",
            "726b8e27185746e2b7ab1ab1ab9cc3ba",
            "2aba54e573ce4427b35b5e99a5ded38b",
            "7cac50fd3d704cb8b67c182658e85bda",
            "f633900cae444a769ea816e35551fe99",
            "e6ebcffc73394b2a88c8ba6853c30984",
            "756a319a4a5244468871186a61010381",
            "3a5bce2ca7f84851afc38c552995f503",
            "32e67e8a3e26415c9446a2fbe7d89e6e",
            "4e6b7b5337854852b62915f888898db3",
            "1a490b14fac24258818a80cd2b9396be",
            "24548b8e27dc44bc932a6cf55880fe3d",
            "fbb95c35deef4658b07c49790b71988d",
            "7659b4fcd04a4096b2f14ffbc850febc",
            "569c071ea10a43a29659471b9c97b968",
            "3745b1d48735480b92a2744b8893b7b0",
            "d88422add32b483ba53f2fe7046824c2",
            "17daa7c0d90c40f3be15a9ce4de586c6",
            "d8636a4e22294123ba4e19a8ef7760fd",
            "8bc0c4c4959648beac092f07cdb2637b",
            "c2993a3ed5bd4972942ea74c23d2b3df",
            "6c7a792bb0d24c7dbeba016f6dc78ff4",
            "db256f74eb0d4b1497717266cab19f75",
            "658b78bc6bcf41819576f3193fe7b0d2",
            "71b5c0b3105749b6b14de3bcbd658e01",
            "c9cf4bae297f41a8abf538202742b78e",
            "3cbd5b58d32a4417905264664f6038ec",
            "0896294f84b04e31a8359ea4cfc58e46",
            "75cd578fa0e34c4fb07ff0f8b2a891d9",
            "cffbc68f22234e05adb0b7aa7836f79c",
            "0043656b6d8c486298b13b2eef3ff2d5",
            "d2b6d00e6bc24131a6ae419f9aa3f362",
            "238c04a2a0e646069ef265d926ed8275",
            "12194054769e4d868c146d17892f8548",
            "a6b0466e6f524d4da7ce8d865ca4b94f",
            "38e948984c494b6bb0f537c88bee52e2",
            "06e2d976fa1c442dbafbbaa7cf21ad9f",
            "02b4332784f2412d8c05084cc37d1d72",
            "4a7accdf7f684cf39871ced66e6319f7",
            "b440a6c43a3240eeb5965732d870c52d",
            "d767ddccf5ea49e2b24902ac1e651128",
            "a91fc82b3c41405da6d3496e3341bb94",
            "5ecc63b0b6a54d1a9b3bfbea52c4ea56",
            "eec2a77f5028412fac2a72b55e0c96ab",
            "bc53a1b85b12488ca910b9a408350efd",
            "778f81c5abed463f8b031f57aebc96e1",
            "9e7a5126de704bde9b98084f45f34656",
            "930567649fc9410985fc5f43bdabbffd",
            "7981ae9da51a4a69b68469f18ed656a6",
            "7b1f36e0a6bb4471b432cd3fae13ee5b",
            "7e5d1af6c78d44db9127a80be737e500",
            "d7e7e03540ee40149a2a4c177984585d",
            "f91d5613dc14413fbd1a5abd590ac7cf",
            "c631617020c64a4c833c017c40fbb33d",
            "7af2f36e2a0a470db0271cda7348a699",
            "d7340441206044eeac6208b7088325c9",
            "334a12de695342e8989308bf6b7ca8fe",
            "f0d5b64b0f9d43639957ece74b32d144",
            "84a997754bdf40708e53875bac9f7f5e",
            "076fb19e5d884a838225efcfc46dfe5d",
            "7b4df1b5735d4f68b1d286a4e82a81eb",
            "fc25a354fc054e9c92bfb403fce0fda8"
          ]
        },
        "id": "YEPEZEsxjMUC",
        "outputId": "007f7940-f8f7-43c4-9738-09d12d4bb597"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad813596e9ba43939ed06620af5c50fe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/56.4k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cfbf3decac3b402a819e81f48041d4f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c629eef4f1e4cbe84920b6f2fe14202"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b003d6be599449c5b9ed4f1a8e2d4f99"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/429M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5cfa91a75d8b43a183268d703c076f31"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a05bb468631a4e38ace9fc28077972dc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/272 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "06764f72b3d34cddb55c8fd742afaf88"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/347 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a36c6df9bbc49aebe4ed916c9d3489b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a5bce2ca7f84851afc38c552995f503"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d8636a4e22294123ba4e19a8ef7760fd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cffbc68f22234e05adb0b7aa7836f79c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d767ddccf5ea49e2b24902ac1e651128"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "chat_template.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d7e7e03540ee40149a2a4c177984585d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\n",
        "                \"type\": \"video\",\n",
        "                \"video\": \"/content/bird.mp4\",\n",
        "                \"max_pixels\": 360 * 420,\n",
        "                \"fps\": 1.0,\n",
        "            },\n",
        "            {\"type\": \"text\", \"text\": \"what is going in this scene\"},\n",
        "        ],\n",
        "    }\n",
        "]\n",
        "\n",
        "# Preparation for inference\n",
        "text = processor.apply_chat_template(\n",
        "    messages, tokenize=False, add_generation_prompt=True\n",
        ")\n",
        "image_inputs, video_inputs = process_vision_info(messages)\n",
        "inputs = processor(\n",
        "    text=[text],\n",
        "    images=image_inputs,\n",
        "    videos=video_inputs,\n",
        "    padding=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "inputs = inputs.to(\"cuda\")\n",
        "\n",
        "# Inference\n",
        "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
        "generated_ids_trimmed = [\n",
        "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
        "]\n",
        "output_text = processor.batch_decode(\n",
        "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        ")\n",
        "print(output_text[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_jeIA4Lqaq-",
        "outputId": "0af54a12-07a1-457b-d6c0-9766a7ac66e5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the video, we see a group of birds perched on tall grasses in a field. The birds are likely feeding on the grasses, as they are seen eating and flitting around the plants. The background is a blurred view of the field, which suggests that the focus is on the birds and their activity. The video appears to be a nature documentary or wildlife video, and the birds are likely a part of the ecosystem in this environment.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s_4vwZYmuF91"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
